<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>《统计学习方法》笔记(三) - 已停更的小博客</title><meta name=renderer content=webkit><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=MobileOptimized content=width><meta name=HandheldFriendly content=true><meta name=applicable-device content=pc,mobile><meta name=theme-color content=#f8f5ec><meta name=msapplication-navbutton-color content=#f8f5ec><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=#f8f5ec><meta name=mobile-web-app-capable content=yes><meta name=author content=Quanyin><meta name=description content=本文是在学习李航老师的《统计学习方法》时做的学习笔记系列的第三篇：第三章-k近邻法><meta name=keywords content="Machine Learning,统计学习方法,笔记,机器学习,k近邻"><link rel=canonical href=/2018/statistical-learning-method-chpt-3/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.css media=screen crossorigin=anonymous><meta property=og:title content=《统计学习方法》笔记(三)><meta property=og:description content=本文是在学习李航老师的《统计学习方法》时做的学习笔记系列的第三篇：第三章-k近邻法><meta property=og:type content=article><meta property=og:url content=/2018/statistical-learning-method-chpt-3/><meta property=article:published_time content=2018-03-29T21:41:22&#43;08:00><meta property=article:modified_time content=2020-05-07T17:24:00&#43;08:00><meta itemprop=name content=《统计学习方法》笔记(三)><meta itemprop=description content=本文是在学习李航老师的《统计学习方法》时做的学习笔记系列的第三篇：第三章-k近邻法><meta itemprop=datePublished content=2018-03-29T21:41:22&#43;08:00><meta itemprop=dateModified content=2020-05-07T17:24:00&#43;08:00><meta itemprop=wordCount content=2510><meta itemprop=keywords content="Machine Learning,"><meta name=twitter:card content=summary><meta name=twitter:title content=《统计学习方法》笔记(三)><meta name=twitter:description content=本文是在学习李航老师的《统计学习方法》时做的学习笔记系列的第三篇：第三章-k近邻法><script>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-126038371-2','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script async src=//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({google_ad_client:"ca-pub-4469282388984999",enable_page_level_ads:true});</script><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=/>主页</a><li class=mobile-menu-item><a class=menu-item-link href=/notice/>通知</a><li class=mobile-menu-item><a class=menu-item-link href=/post/>时光机</a><li class=mobile-menu-item><div class=mobile-menu-parent><span class=mobile-submenu-open></span><a href=/categories/>分类</a></div><ul class=mobile-submenu-list><li><a href=/categories/%E7%AC%94%E8%AE%B0>笔记</a><li><a href=/categories/%E7%BC%96%E7%A8%8B>编程</a><li><a href=/categories/%E7%A7%91%E7%A0%94>科研</a><li><a href=/categories/%E5%AD%A6%E4%B9%A0>学习</a><li><a href=/categories/%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86>错误处理</a></ul><li class=mobile-menu-item><a class=menu-item-link href=/about/>关于我</a></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>Blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>主页</a><li class=menu-item><a class=menu-item-link href=/notice/>通知</a><li class=menu-item><a class=menu-item-link href=/post/>时光机</a><li class=menu-item><a class="menu-item-link menu-parent" href=/categories/>分类</a><ul class=submenu><li><a href=/categories/%E7%AC%94%E8%AE%B0>笔记</a><li><a href=/categories/%E7%BC%96%E7%A8%8B>编程</a><li><a href=/categories/%E7%A7%91%E7%A0%94>科研</a><li><a href=/categories/%E5%AD%A6%E4%B9%A0>学习</a><li><a href=/categories/%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86>错误处理</a></ul><li class=menu-item><a class=menu-item-link href=/about/>关于我</a></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>《统计学习方法》笔记(三)</h1><div class=post-meta><time datetime=2018-03-29 class=post-time>2018-03-29</time><div class=post-category><a href=/categories/%E7%AC%94%E8%AE%B0/>笔记</a></div><span class=more-meta>约 2510 字</span>
<span class=more-meta>预计阅读 6 分钟</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#一-k-近邻算法>一、k 近邻算法</a><li><a href=#二-k-近邻模型>二、k 近邻模型</a><ul><li><a href=#模型>模型</a><li><a href=#距离度量>距离度量</a><li><a href=#k-值的选取>k 值的选取</a><li><a href=#分类决策规则>分类决策规则</a></ul><li><a href=#三-k-近邻的实现-kd-树>三、k 近邻的实现：kd 树</a><ul><li><a href=#构造>构造</a><li><a href=#搜索>搜索</a></ul></ul></ul></nav></div></div><div class=post-content><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-4469282388984999 data-ad-slot=4019390506 data-ad-format=link data-full-width-responsive=true></ins><p>k 近邻法是一种基本分类与回归方法，其输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。
k 近邻法利用训练数据集对特征空间进行划分，并作为其分类的 “模型”。k 值的选择，距离度量及
分类决策规则是 k 近邻法的三个基本要素。k 近邻法 1968 年由 Cover 和 Hart 提出。<h2 id=一-k-近邻算法>一、k 近邻算法</h2><p>给定一个训练数据集，对新的输入实例，在数据集中找到与该实例最近的 $k$ 个实例，这 $k$ 个实例，
实例的多数属于某个类，就把该输入实例分为这个类。k 近邻没有显式的学习过程。<p>输入：训练数据集<code>$ T=\{(x_1,y_1),...,(x_N,y_N)\}$</code>
输出：实例所属的类别 $y$<ol><li>根据给定的距离度量，在训练集中找出与 $x$ 最近的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的领域记作$ N_k(x) $;<li>在$ N_k(x) $ 中根据分类决策规则(如多数表决)决定x的类别y：
<code>$$ y = arg \max\limits_{c_j}\sum\limits_{x_i \in N_k(x)} I(y_i=c_j)，i=1,2,...N;
j=1,2,...,K$$</code>
其中，$I$ 为指示函数，即当$y_i=c_j$时取1，否则为0.</ol><h2 id=二-k-近邻模型>二、k 近邻模型</h2><h3 id=模型>模型</h3><p>k 近邻法使用的模型实际上对应于特征空间的划分。模型由三个基本要素——距离度量、k 值的选择和分类决策规则决定。
当三者确定后，对于任何一个输入实例，它所属的类唯一确定。<p>特征空间中，对每个训练实例点 $x_i$，距离该点比其他更近的所有的点组成一个区域，叫做单元(cell)；每个训练实例
点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。<p><link rel=stylesheet href=/css/hugo-easy-gallery.css><div class=box><figure class="center lazyload" itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/img/post-content/statistic-learning-method/post-Lihang-chpt3-model.png alt=k近邻法的模型对应特征空间的一个划分></div><a href=/img/post-content/statistic-learning-method/post-Lihang-chpt3-model.png itemprop=contentUrl></a><figcaption><h4>k近邻法的模型对应特征空间的一个划分</h4></figcaption></figure></div><h3 id=距离度量>距离度量</h3><p>特征空间中两个实例点的距离是两个实例点相似程度的反映。k 近邻模型的特征空间一般是 n 维
实数向量空间 $R^n$。使用的距离是欧式距离或者更一般的$L_p$距离。
<code>$$ L_p(x_i,x_j)=(\sum\limits_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}} $$</code>
这里 $p \ge 1$。当 $p=1$时，称为曼哈顿距离；当 $p=2$时，即为熟知的欧氏距离；当 $p=\infty$时，它是各个坐标距离的
最大值。<div class=box><figure class="center lazyload" itemprop=associatedMedia itemscope itemtype=http://schema.org/ImageObject><div class=img><img itemprop=thumbnail src=/img/post-content/statistic-learning-method/post-Lihang-chpt3-L_distance.png alt=L_p距离间的关系></div><a href=/img/post-content/statistic-learning-method/post-Lihang-chpt3-L_distance.png itemprop=contentUrl></a><figcaption><h4>L_p距离间的关系</h4></figcaption></figure></div><h3 id=k-值的选取>k 值的选取</h3><p>k 值的选择会对 k 近邻的算法结果产生重大影响。<p>选择较小的 k 值，就相当于用较小的邻域中的训练实例进行预测，学习的近似误差会减小 只有与输入实例较近的训练实例才会起作用，但缺点是学习的估计误差会增大，即，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。<p>选择较大的 k 值，就相当于用较大邻域中的训练实例进行预测。其优点是减少学习的估计误差，但缺点是学习的近似误差会增大。即，k 值的增大意味着整体的模型变得简单，容易发生欠拟合。<p>在应用中，k 值一般取一个比较小的数值，通常采用交叉验证法来选取最优的 k 值。<h3 id=分类决策规则>分类决策规则</h3><p>k 近邻法中的分类决策规则往往是多数表决，即由输入实例的 k 个邻近的训练实例中的多数
类决定输入实例的类。<p>多数表决规则等价于采用指示函数时的经验风险最小化。<h2 id=三-k-近邻的实现-kd-树>三、k 近邻的实现：kd 树</h2><p>实现 k 近邻法时，主要考虑的问题是如何对训练数据进行快速 k 近邻搜索。
kd 树是一种对 k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。
kd 树是二叉树，表示对 k 维空间的一个划分。<h3 id=构造>构造</h3><p>构造 kd 树相当于不端的用垂直于坐标轴的超平面将 k 维平面切分，构成一系列的 k 维超矩形区域。kd 树的每个结点对应于一个 k 维超矩形区域。<p>构造方法如下：<ul><li>构造根结点，使根结点对应于 k 维空间中包含所有实例点的超矩形区域；<li>通过下面的递归方法，不断的对 k 维空间进行切分，生成子结点。<ul><li>在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，
这个超平面通过选取的切分点并垂直于选定的坐标轴；<li>上述超平面将当前区域划分为左右两个子区域(子结点)，实例被分到两个子区域；<li>直到子区域内没有实例时终止(终止时的结点为页结点)<br></ul></ul><p><strong>构造平衡 kd 树(切分点选为中位数)具体算法：</strong><p>输入：k 维空间数据集 <code>$ T=\{x_1,...,x_N\} $</code>，其中 $ x_i = (x_i^{(1)},x_i^{(2)},&hellip;,x_i^{(k)})^T $<p>输出：kd 树<ol><li>开始：<ul><li>构造根结点，根结点对应于包含 T 的 k 维空间的超矩形区域。<li>选择 $x^{(1)}$ 为坐标轴，以 $T$中所有实例的 $x^{(1)}$ 坐标的中位数为切分点，将根结点对应的
超矩形区域切分为两个子区域。切分由通过切分点并与做坐标轴 $x^{(1)}$ 垂直的超平面实现。<li>由根结点生成深度为 1 的左右子结点，分别对应坐标 $x^{(1)}$ 小于、大于切分点的子区域。<li>将落在切分超平面上的实例点保存在根结点。</ul><li>重复：<ul><li>对深度为 j 的结点，选择 $x^{(l)}$ 为切分的坐标轴，$l=j(mod k)+1 $，以该
结点的区域中所有实例的 $x^{(l)}$ 坐标的中位数为切分点，将该结点对应的超矩形区域
切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(l)}$ 垂直的超平面实现。<li>由该结点生成深度为 j+1 的左右子结点，分别对应坐标；<li>将落在切分超平面上的实例点保存在该结点。</ul><li>结束：<ul><li>直到两个子区域没有实例存在时停止，从而形成 kd 树的区域划分。</ul></ol><p><img src=/img/post-content/statistic-learning-method/post-Lihang-chpt3-space.png alt=特征空间划分>
<img src=/img/post-content/statistic-learning-method/post-Lihang-chpt3-kdtree.png alt="kd 树示例"><h3 id=搜索>搜索</h3><p>以最近邻为例，给定一个目标点，搜索其最近邻：<ol><li>首先找到包含该目标点的叶结点；<li>然后从该叶结点出发，依次退回到父结点；<li>不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。<ul><li>包含目标点的叶结点对应包含目标点的最小超矩形区域；<li>以此叶结点的实例点为当前最近点，目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部，然后返回当前结点的父结点，<ul><li>如果父结点的另一子结点的超矩形区域与超球体相交，则在相交的区域内寻找与目标点更近的实例点，如果存在这样的点，将此点作为新的当前最近点，算法转到更上一级的父结点，继续上述过程；<li>如果父结点的另一子结点的超矩形区域与超球体不相交，或不存在比当前更近的点，则停止搜索。</ul></ul></ol><p><strong>用 kd 树的最近邻搜索算法：</strong><p>输入：已构造的 kd 树；目标点 x ；
输出：x 的最近邻<ol><li>在 kd 树中找到包含目标点 x 的叶结点：从根结点出发，递归的向下访问 kd 树。若目标
点 x 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移到右子结点。直到子结点为叶结点为止；<li>以此叶结点为“当前最近点”；<li>递归的向上回退，在每个结点进行一下操作：<ul><li>如果该结点保存的实例点比当前最近点距离目标点更加，则以该实例点为“当前最近点”<li>当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超秋体相交：<ul><li>如果相交，可能在另一子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。递归的进行最近邻搜索；<li>如果不相交，向上回退</ul></ul><li>当回退到根结点时搜索结束。最后的“当前最近点”即为 x 的最近邻点。</ol><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-4469282388984999 data-ad-slot=5357438236></ins><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-4469282388984999 data-ad-slot=5625031544 data-ad-format=auto data-full-width-responsive=true></ins></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>Quanyin</span><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2020-05-07</span><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content>本博客所有文章除特别声明外，均采用 <a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>(CC) BY-NC-SA 4.0 </a>许可协议。转载请注明作者和出处并告知</span></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/img/reward/WechatPay.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/img/reward/AliPay.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags><a href=/tags/machine-learning/>Machine Learning</a></div><nav class=post-nav><a class=prev href=/2018/03-focus-on-content/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">专注于内容</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/2018/statistical-learning-method-chpt-2/><span class="next-text nav-default">《统计学习方法》笔记(二)</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"/></svg></i></a></nav></footer></article><div class="post bg-white"><div id=comments></div><script src=//cdn1.lncld.net/static/js/3.0.4/av-min.js></script><script src=//unpkg.com/valine/dist/Valine.min.js></script><script>if(window.location.hash){var checkExist=setInterval(function(){if($(window.location.hash).length){$('html, body').animate({scrollTop:$(window.location.hash).offset().top-90},700);clearInterval(checkExist);}},10);}</script><script>new Valine({el:'#comments',appId:'aTnk2nRIOVuFeXVYCRqOhEQ1-MdYXbMMI',appKey:'fPBawjKkMiA8biHbwoDim1mJ',notify:false,verify:false,avatar:'mm',placeholder:'说点什么吧',visitor:false});</script></div></div></div></main><footer id=footer class=footer><div class=icon-links><a href=/index.xml rel="noopener alternate" type=application/rss&#43;xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 0 1 140.501333 140.586667A140.928 140.928.0 0 1 140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2017 -
2020
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>Quanyin</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777v0zm0 0"/></svg></i></div></div><script src=/lib/jquery/jquery-3.2.1.min.js></script><script src=/lib/slideout/slideout-1.0.1.min.js></script><script src=/js/main.js crossorigin=anonymous></script><script>window.MathJax={showProcessingMessages:false,messageStyle:'none'};</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script src=/js/load-photoswipe.js></script><script src=/lib/photoswipe/photoswipe.min.js></script><script src=/lib/photoswipe/photoswipe-ui-default.min.js></script><script>(adsbygoogle=window.adsbygoogle||[]).push({});</script>